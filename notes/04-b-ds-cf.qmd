---
title: | 
    | Data Science Methods in Causal Inference
author: "Ryan T. Moore"
date: 2024-08-23
date-format: iso
execute: 
  echo: true
format: 
  beamer:
    fonttheme: serif
    include-in-header:
      - text: | 
          \usepackage{amsmath}
          \usepackage{wasysym}
          \newcommand{\independent}{\perp\mkern-9.5mu\perp}
          \setbeamertemplate{footline}[page number]
    section-titles: true
    toc: true
institute:
  - American University
bibliography: "../admin/main.bib"
---

```{r}
#| label: setup
#| echo: false
#| message: false
#| results: false
#| warning: false

library(broom)
library(dagitty)
library(estimatr)
library(ggdag)
library(here)
library(tidyverse)
```

# Data Science in Causal Inference

## Approaches of Prediction and Causal Inference

\large

_Two Cultures_, [@breiman01-two]

>- _Data Models_: our "social science modeling"
>- _Algorithmic Models_: our "data science algorithms"

## Methods for Prediction and Causal Inference

\Large

- Cross-validation
- Regression/Decision trees
- Random forests

\vspace{5mm}

@jamwithas21

# Cross-validation

## Cross-validation

\large

$k$-fold cross-validation to select method

\pause 

>- Randomly partition data into $k$ groups
>- Apply method to $k-1$ groups
>- Use result to predict for left-out group
>- Calculate $\text{MSE}_i = \frac{1}{n} \sum\limits_{i=1}^n \left(y_i - \hat{y}_i \right)^2$ 
>- Calculate test error as average of the $k$ MSE's: \pause 

$$CV_{(k)} = \frac{1}{k} \sum\limits_{i=1}^k \text{MSE}_i$$
\pause 

>- Select model that minimises $CV_{(k)}$


## CV for Linear Model

```{r}
## Make data

mk_data <- function(n = 90, n_folds = 10){
  
  df <- tibble(
    x1 = rnorm(n),
    x2 = rnorm(n), 
    x3 = rnorm(n), 
    y = 0.1 * x1 + 0.2 * x2 + 0.5 * x3 + rnorm(n),
    cv_fold = sample(rep(1:n_folds, (n / n_folds)))
  )
  
}

df <- mk_data()
```

## CV for Linear Model

```{r}
head(df)
```

\pause 

```{r}
table(df$cv_fold)
```


## CV for Linear Model

```{r}
cv_lm <- function(data, fmla){
  
  n_folds <- max(data$cv_fold)
  store_mses <- vector("numeric", length = n_folds)
  
  for(idx in 1:n_folds){
    
    df_train <- data |> filter(cv_fold != idx)
    df_test <- data |> filter(cv_fold == idx)
  
    lm_out <- lm(fmla, data = df_train)
  
    predictions <- predict(lm_out, newdata = df_test)
    
    store_mses[idx] <- mean((df_test$y - predictions)^2)}

  test_error_cv_k <- mean(store_mses)
  return(test_error_cv_k)
}
```

## CV for Linear Model

\large

```{r}
cv_lm(data = df, fmla = y ~ x1 + x2)
```

\pause 

```{r}
df <- mk_data()
cv_lm(df, y ~ x1 + x2)
```

\pause 

```{r}
df <- mk_data()
cv_lm(df, y ~ x1 + x2 + x3)
```

## CV for Linear Model

```{r}
#| echo: false
#| fig-cap: "MSE always less (better) for 3-variable model."
#| fig-height: 6

set.seed(631537784)

n_iter <- 100

mses_2var <- mses_3var <- vector("numeric", length = 100)

for(idx in 1:n_iter){
  df <- mk_data()
  mses_2var[idx] <- cv_lm(df, y ~ x1 + x2)
  mses_3var[idx] <- cv_lm(df, y ~ x1 + x2 + x3)
}

mses <- tibble(mses_2var, 
               mses_3var, 
               difference = mses_3var - mses_2var)

ggplot(mses, aes(difference)) + 
  geom_histogram() + 
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(x = "(MSE 3-var model) - (MSE 2-var model). 100 data sets.")

# Long version:
# mses <- mses |> pivot_longer(cols = starts_with("mses"),
#                              names_to = "model") |>
#   mutate(model = case_when(model == "mses_2var" ~ "y ~ x1 + x2", TRUE ~ "y ~ x1 + x2 + x3"))

# ggplot(mses, aes(value, model)) + 
#  geom_violin(draw_quantiles = c(.25, .5, .75))
```

# Regression Trees

## Regression Trees

\large

>- Partition predictor space into regions $R_1, R_2, \ldots, R_J$.
>- If unit falls in region $R_j$, use average outcome in $R_j$ as predicted value: $\hat{y}_{R_j}$
>- (For _decision_ on discrete outcome, count votes in $R_j$)
>- Goal: minimise residual sum of squares (RSS), just like LS regression: \pause 

$$\sum\limits_{j=1}^J \sum\limits_{i \in R_j} \left(y_i - \hat{y}_{R_j}\right)$$

## Regression Trees

\large


How to define regions $R_j$?

\pause 

>- Top-down, greedy recursive binary split
>- At each step, find predictor and cut-point that minimise \pause 

$$\sum\limits_{i: x\in R_1(j,s)} \left(y_i - \hat{y}_{R_1(j,s)}\right)^2 +
\sum\limits_{i:x \in R_2(j,s)}  \left(y_i - \hat{y}_{R_2(j,s)}\right)^2$$

## Regression Trees

\Large

>- Overfitting is a potential problem 
>- Can we increase predictive quality by only using _part_ of a tree?
>- "Pruning"

## Regression Trees

Pruning

>- Build a large tree
>- Select the subtree that gives least prediction error  
(via cross-validation)
>- But, many possible subtrees, so penalise larger trees via $\alpha$
>- $\alpha$: penalty parameter
>- $|T|$: count of terminal nodes of $T$
>- $m$: terminal node index
>- Find subtree that minimises \pause 

$$\sum\limits_{m=1}^{|T|} \sum\limits_{i:x_i \in R_m} \left(y_i - \hat{y}_{R_m} \right)^2 + \alpha |T|$$
\pause 

Sum squared pred. error (plus penalty that grows with tree size) across units in region, then regions. 

## Regression Trees

\large

But, how to choose $\alpha$? (Use cross-validation.)

1. Build big tree on training data (with some minimum terminal node size) \pause 
2. For several values of $\alpha$, find best subtree. \pause 
3. Find best value of $\alpha$ via CV. Create $K$ folds. Then \pause \newline
3a. Do 1 and 2 on all but $k$th fold  \pause  
\newline 
3b. Predict for $k$th fold, calculate MSE for several values of $\alpha$  \pause  \newline
3c. Get avg MSE for each $\alpha$  \pause \newline
3d. Pick $\alpha$ to minimise MSE  \pause \newline
4. Using that $\alpha$, select best subtree from Step 2

## Example: Regression Tree

Effect of office-holding on wealth  
[@egghai09]:

```{r}
library(qss)
library(rsample)
library(tree)

data("MPs")
mps <- MPs |> mutate(age = yod - yob,
                     is_labour = if_else(party == "labour", 1, 0),
                     is_london = if_else(region == "Greater London", 1, 0), 
                     is_winner = if_else(margin > 0, 1, 0)) |>
  select(ln.net, age, is_labour, is_london, is_winner) |> 
  na.omit()
```


## Example: Regression Tree

\large

```{r}
set.seed(765076184)

mp_split <- initial_split(mps, prop = 0.7)

mp_train <- training(mp_split)
mp_test <- testing(mp_split)
```


## Example: Regression Tree

```{r}
#| fig-cap: "The regression tree (for training data)"
#| fig-height: 5
tree_mp <- tree(ln.net ~ ., data = mp_train)
plot(tree_mp)
text(tree_mp)
```

## Example: Regression Tree

\large
Would pruning help?

\pause 

```{r}
#| fig-cap: "Subtree size 2 minimises SSR"
#| fig-height: 4
cv_mps <- cv.tree(tree_mp, K = 10)

plot(cv_mps$size, cv_mps$dev, type = "b")
```

\pause 
Yes. So prune at $|T| = 2$.

## Example: Regression Tree

```{r}
#| fig-cap: "The pruned tree"
#| fig-height: 3

prune_mps <- prune.tree(tree_mp, best = 2)

plot(prune_mps)
text(prune_mps)
```

## Example: Regression Tree

\large

Predict for test set:

```{r}
#| echo: false
mps_preds_full <- predict(tree_mp, newdata = mp_test)
mps_preds_prune <- predict(prune_mps, newdata = mp_test)

df_mp_preds <- tibble(
  preds_full = mps_preds_full,
  preds_prune = mps_preds_prune,
  observed = mp_test$ln.net
)

# pl_full <- ggplot(df_mp_preds, aes(preds_full, observed)) + geom_point() 

# pl_full +
#   geom_point(aes(preds_prune, observed), shape = 2)
```

```{r}
#| echo: false
mse_full <- mean((df_mp_preds$observed - df_mp_preds$preds_full)^2)

mse_prune <- mean((df_mp_preds$observed - df_mp_preds$preds_prune)^2)
```

- MSE for pruned: `r round(mse_prune, 3)`
- MSE for full: `r round(mse_full, 3)`

\pause 

So, pruning helped us avoid some overfitting. 

\pause 

(Typical pred error of $\sqrt{`r round(mse_prune, 3)`} \approx `r round(sqrt(mse_prune), 3)`$)

\pause 
(Bigger than IQR of `r round(IQR(mps$ln.net), 3)`, but range covers $\left[ `r round(min(mps$ln.net), 2)`, `r round(max(mps$ln.net), 2)`
 \right]$.)
 
 \pause 
(Pretty good for 1 split!?)

# Random Forests

## Random Forests

\large

Next: random forest algorithm

\pause 

Ensemble learning algorithms:

\pause 

>- Boosting: models build on prior models $\rightsquigarrow$ pick feature, predict, upweight mispredicted data, .... Do several times and combine.
>- Bagging: (random select units, model) $\to$ many times. No building.

\pause 
Random Forests are bagging algorithms.

\pause 

_Bagging_: \textit{b}ootstrap \textit{agg}regation

## Random Forests

\Large
Why bag?

\pause 

>- Trees are low bias, high variance  
(diff answers, depend on data split)
>- Bagging averages over data subsets, reducing variance
>- (Linear regression: lower variance)

## Random Forests

\Large
Random forests: decorrelated, bagged trees

\pause 

>- Take bootstrapped training subsample
>- Build deep tree. At each split, _randomly sample_ $m$ of $p$ predictors, build split from only those $m$.
>- (Often choose $m \approx \sqrt{p}$)
>- So, different splits consider different predictors
>- So, trees will look very different to each other

## Example: Random Forests

\large

```{r}
library(randomForest)

# Full bag:
bag_mps <- randomForest(ln.net ~ ., data = mp_train,
                        ntree = 500, mtry = 4, 
                        importance = TRUE)

# Decorrelate:
rf_mps <- randomForest(ln.net ~ ., data = mp_train,
                       ntree = 500, mtry = 2, 
                       importance = TRUE)
```

## Example: Random Forests

\large

Predict:
```{r}
preds_bag <- predict(bag_mps, newdata = mp_test)

preds_rf <- predict(rf_mps, newdata = mp_test)
```

```{r}
#| echo: false

mse_bag <- mean((mp_test$ln.net - preds_bag)^2)

mse_rf <- mean((mp_test$ln.net - preds_rf)^2)
```

- MSE for RF: `r round(mse_rf, 3)`
- MSE for full bag: `r round(mse_bag, 3)`

\pause 

So, decorrelating helped us avoid some overfitting to each bootstrap subsample (and thus, reduced variance).

\pause 

(Typical pred error of $\sqrt{`r round(mse_rf, 3)`} \approx `r round(sqrt(mse_rf), 3)`$)

\pause 
(Bigger than IQR of `r round(IQR(mps$ln.net), 3)`, but range covers $\left[ `r round(min(mps$ln.net), 2)`, `r round(max(mps$ln.net), 2)`
 \right]$.)

# Heterogeneous Treatment Effects

## Homogeneous and Heterogeneous Effects

\large

>- Most causal inference starts at _average treatment effects_
>- Average may be interesting on its own, \ldots \pause but often masks assumption of _homogeneous_ effects \pause 
>- Notationally, often assume $\tau_i = \tau \quad \forall i$
>- But, _heterogeneous_ effects often of central interest
>- Different effects for different groups
>    - Subgroup variability (research)
>    - Targeting resources (campaigns, marketing)
>    - Constituency effects (public policy)
>- Notationally, $\exists i: \tau_i \neq \tau$

## Homogeneous and Heterogeneous Effects: Estimation

Homogeneous effects:

$$\text{Outcome} = \beta_0 + \beta_1 \text{Treatment} + \epsilon$$

\pause 

```{r}
lm_out <- lm(ln.net ~ is_winner, data = mps)
lm_out
```

## Homogeneous and Heterogeneous Effects: Estimation

\large 

Homogeneous effects:

```{r}
t.test(ln.net ~ is_winner, data = mps)
```


## Homogeneous and Heterogeneous Effects: Estimation

Homogeneous effects:

$$\text{Outcome} = \beta_0 + \beta_1 \text{Treatment} + \sum \beta_j X_j +  \epsilon$$

\pause 

```{r}
lm_out <- lm(ln.net ~ is_winner + is_labour + 
               is_london + age, data = mps)
lm_out
```

## Homogeneous and Heterogeneous Effects: Estimation

Homogeneous effects:

```{r}
lm_lin(ln.net ~ is_winner, covariates = ~ is_labour + is_london + age, data = mps)
```



<!-- ## Homogeneous and Heterogeneous Effects: Detection -->

<!-- Heterogeneous effects: -->

## CATEs: Conditional ATEs

\Large

>- _Conditional average treatment effect_ (CATE): avg treatment effect for subset of population
>- Sometimes "CACE"
>- Inference: not "evidence against $\text{TE} = 0$?", but "evidence against $\text{CATE}_1 = \text{CATE}_2$?"

## Homogeneous and Heterogeneous Effects: Estimation

Heterogeneous effects:

$$\text{Outcome} = \beta_0 + \beta_1 \text{Treatment} + \beta_2 \text{Group} + \beta_3 \text{Treatment} \cdot \text{Group}  + \epsilon$$

\pause 
>- $\beta_1$ gives TE for `Group == 0` 
>- $\beta_1 + \beta_3$ gives TE for `Group == 1` 

## Homogeneous and Heterogeneous Effects: Estimation

Heterogeneous effects:

```{r}
lm_out <- lm(ln.net ~ is_winner * is_labour + 
               is_london + age, data = mps)
coef(lm_out) |> round(3)
```

# Causal Forests

## Causal Forests

\large

>- Our regression trees had terminal nodes ("leaves") that were sufficiently homogeneous for prediction.
>- Use $\hat{y}_{R_j}$ as pred value for obs in $R_j$
>- (Tory, winner, London $\to 13.84$)
>- $$\hat{y}_{R_j} = \frac{1}{|R_j|} \sum\limits_{i \in R_j} Y_i$$

## Causal Forests

\large

>- Similarly, consider each leaf $R_j$ small, homogeneous enough that potential outcomes independent
>- Treateds in $R_j$ provide good estimates of what controls in $R_j$ would have done under treatment
>- Controls in $R_j$ provide good estimates of what treateds in $R_j$ would have done under control
>- Assignment w/in leaf $R_j$ is as-good-as-random
>- I.e., each leaf contains an experiment

\pause 

$$Y(0), Y(1) \independent T | \bf X $$

## Causal Forests

\large

>- Let $\{T, R_j\} = \{i: T_i = 1, i \in R_j \}$ \quad (Tr obs in $R_j$)
>- Let $\{C, R_j\} = \{i: T_i = 0, i \in R_j \}$ \quad (Co obs in $R_j$)
>- Natural estimation of \pause 

$$\hat{\bar{\tau}}_{R_j} = \frac{1}{|\{T, R_j\}|} \sum\limits_{\{T, R_j \}} Y_i -  \frac{1}{|\{C, R_j\}|} \sum\limits_{\{C, R_j \}} Y_i$$

\pause 
So, we can use RF methods to estimate conditional (heterogeneous) treatment effects, CATEs.

\pause 

\center
\smiley


## Causal Forests: Honesty

\large

>- But need one more thing for asymptotics to work out \ldots
>- Each tree must be _honest_
>- Each unit $i$ _either_ 
>    - used to determine tree splits, _or_
>    - used to estimate $\hat{\bar{\tau}}_{R_j}$
>- But **not both**!
>- One way: "Double-sample" causal trees
>    - Split training data into $\mathcal{I}$ and $\mathcal{J}$
>    - Splits chosen to maximise variance on $\hat{\bar{\tau}} \text{ for } i \in \mathcal{J}$
>    - Splitting cannot use $y_i$ from $\mathcal{I}$
>    - Prediction, estimation of $\hat{\bar{\tau}}$ uses only $\mathcal{I}$
>- Build a random forest (decorrelated deep trees picking from $m$ predictors) of causal trees

## Example: Causal Forests

```{r}
#| eval: true

library(grf)

X <- mps |> select(age, is_labour, is_london)

W <- mps |> select(is_winner) |> 
  unlist() |> as.numeric() 

Y <- mps |> select(ln.net) |> unlist() 

cf_out <- causal_forest(X, Y, W)
```

## Example: Causal Forests

\large

```{r}
cf_out
```

\pause 

("How frequently was $i$ the split feature?")

## Example: Causal Forests

\large 

```{r}
cf_pred_est_var <- predict(cf_out, X, 
                           estimate.variance = TRUE)
```

\pause 

```{r}
cf_preds <- cf_pred_est_var$predictions

df_cf <- tibble(X, 
                cf_te = cf_preds,
                cf_se = sqrt(cf_pred_est_var$variance.estimates),
                te_1se_lower = cf_te - cf_se,
                te_1se_upper = cf_te + cf_se)
```

## Example: Causal Forests

\Large

Avg pred treatment effect in honest sample:

```{r}
mean(cf_preds)
```

## Example: Causal Forests

\Large

A doubly-robust ATE from honest sample:

```{r}
average_treatment_effect(cf_out)
```


## Example: Causal Forests Results, Party

```{r}
#| echo: false
#| fig-height: 6

ggplot(df_cf, aes(factor(is_labour), cf_te)) + 
  geom_violin(draw_quantiles = (1:3) / 4) +
  labs(x = "Is Labour?", y = "Honest CF Estimated Treatment Effect")

tory_mean_cf_te <- mean(df_cf$cf_te[df_cf$is_labour == 0])

labour_mean_cf_te <- mean(df_cf$cf_te[df_cf$is_labour == 1])
```

\pause 
- Mean CF TE, Tory: `r round(tory_mean_cf_te, 3)` \pause $\rightsquigarrow \pounds `r formatC(round(exp(median(mps$ln.net) + tory_mean_cf_te) - exp(median(mps$ln.net)), -3), format = "f", digits = 0, big.mark = ",")`$ \pause 
- Mean CF TE, Labour: `r round(labour_mean_cf_te, 3)` \pause $\rightsquigarrow \pounds `r formatC(round(exp(median(mps$ln.net) + labour_mean_cf_te) - exp(median(mps$ln.net)), -3), format = "f", digits = 0, big.mark = ",")`$ 

(mix of medians/means here \ldots)

## Example: Causal Forests Results, Party

\large

```{r}
average_treatment_effect(
  cf_out, 
  subset = X$is_labour == 0)
average_treatment_effect(
  cf_out, 
  subset = X$is_labour == 1)
```



## Example: Causal Forests Results, London

```{r}
#| echo: false

ggplot(df_cf, aes(factor(is_london), cf_te)) + 
  geom_violin(draw_quantiles = (1:3) / 4) +
  labs(x = "Is London?", y = "Honest CF Estimated Treatment Effect")
```

## Example: Causal Forests Results, London


\large

```{r}
average_treatment_effect(
  cf_out, 
  subset = X[, "is_london"] == 1)
average_treatment_effect(
  cf_out, 
  subset = X[, "is_london"] == 0)
```

## Example: Causal Forests Results, Age

```{r}
#| echo: false

ggplot(df_cf, aes(x = age, y = cf_te)) + 
  geom_point(aes(colour = factor(is_labour))) + 
  geom_smooth(aes(colour = factor(is_labour)), se = FALSE) + 
  labs(x = "Age at Death", y = "Honest CF Estimated Treatment Effect")
```

<!-- ## Example: Causal Forests Results, Age -->

```{r}
#| echo: false
#| eval: false

ggplot(df_cf, aes(x = age, y = cf_te)) + 
  geom_point(aes(color = factor(is_labour))) + 
  geom_errorbar(aes(ymin = te_1se_lower, 
                    ymax = te_1se_upper,
                    color = factor(is_labour))) + 
  labs(x = "Age at Death", y = "Honest CF Estimated Treatment Effect")
```


## Some Next Ideas \ldots

\Large

- Feature Selection
- Regularization/Shrinkage  
(LASSO, ridge, elastic net)
- Double LASSO for treatment effects  
(models for treatment and outcome)

# Conclusions

## The Big Picture

![](figs/04-grid.pdf)

## Final Thought on Importance of Comparison Groups (Tufte 1974)

\pause 

![](figs/04-tufteWhichHalf.png){width=105%}



***

\Large
\centering
Thank you.
\pause 

Your engagement, your ideas, your questions, your participation, your good nature, your stamina (800 minutes!), and your hard work have been a joy to share.

It has been a great honor to teach you this week.

\pause 

Stay in touch.

\begin{center}
\texttt{rtm@american.edu} \\
\texttt{www.ryantmoore.org}
\end{center}


## References {.allowframebreaks}

\footnotesize

